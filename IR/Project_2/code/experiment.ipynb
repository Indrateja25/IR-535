{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5419b14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:40.688889Z",
     "start_time": "2023-10-30T21:50:40.683001Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104efa29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:50.745834Z",
     "start_time": "2023-10-30T21:49:50.732673Z"
    }
   },
   "outputs": [],
   "source": [
    "from linkedlist import LinkedList\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92e0fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:51.472884Z",
     "start_time": "2023-10-30T21:49:51.129005Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0221a529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:51.495243Z",
     "start_time": "2023-10-30T21:49:51.487962Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39a8e454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:52.431603Z",
     "start_time": "2023-10-30T21:49:52.395857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/input_corpus.txt'\n",
    "df = pd.read_csv(filename, sep='\\t',names=['DocID','content'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dace4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:53.079192Z",
     "start_time": "2023-10-30T21:49:53.079180Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    tokens = text.split(' ')\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    \n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69aee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2646c233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:53.265685Z",
     "start_time": "2023-10-30T21:49:53.258228Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.ps = PorterStemmer()\n",
    "\n",
    "    def get_doc_id(self, doc):\n",
    "        \"\"\" Splits each line of the document, into doc_id & text.\n",
    "            Already implemented\"\"\"\n",
    "        arr = doc.split(\"\\t\")\n",
    "        return int(arr[0]), arr[1]\n",
    "\n",
    "    def tokenizer(self, text):\n",
    "        \"\"\" Implement logic to pre-process & tokenize document text.\n",
    "            Write the code in such a way that it can be re-used for processing the user's query.\n",
    "            To be implemented.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        text = text.strip()\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        tokens = text.split(' ')\n",
    "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        stemmed_tokens = [self.ps.stem(token) for token in filtered_tokens]\n",
    "\n",
    "        stemmed_text = ' '.join(stemmed_tokens)\n",
    "        return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feeec1a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:53.526258Z",
     "start_time": "2023-10-30T21:49:53.516374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Preprocessor at 0x7f7d90e541f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Preprocessor()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc3d5a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:53.860379Z",
     "start_time": "2023-10-30T21:49:53.849582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epidemiolog',\n",
       " 'clinic',\n",
       " 'characterist',\n",
       " '136',\n",
       " 'case',\n",
       " 'covid19',\n",
       " 'main',\n",
       " 'district',\n",
       " 'chongq']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Epidemiological and clinical characteristics of 136 cases of COVID-19 in main district of Chongqing'\n",
    "p.tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49eb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87f557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1f96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13de6916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:55.240142Z",
     "start_time": "2023-10-30T21:49:55.233003Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, value=None, next=None, skip_next=None, tf=0):\n",
    "        \"\"\" Class to define the structure of each node in a linked list (postings list).\n",
    "            Value: document id, Next: Pointer to the next node\n",
    "            Add more parameters if needed.\n",
    "            Hint: You may want to define skip pointers & appropriate score calculation here\"\"\"\n",
    "        self.value = value #document-ID\n",
    "        self.next = next #next pointer\n",
    "        self.skip_next = skip_next #skip pointer\n",
    "        self.tf = tf #tf score for each term,doc pair\n",
    "        \n",
    "    def increase_tf_count(self):\n",
    "        self.tf = self.tf+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3a5320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:55.452444Z",
     "start_time": "2023-10-30T21:49:55.446894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Node(5)\n",
    "n.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e9231b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:55.802395Z",
     "start_time": "2023-10-30T21:49:55.796259Z"
    }
   },
   "outputs": [],
   "source": [
    "n.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0788349f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:56.032688Z",
     "start_time": "2023-10-30T21:49:56.023863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Node(6,n)\n",
    "m.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47dc57fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:56.251304Z",
     "start_time": "2023-10-30T21:49:56.242958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Node at 0x7f7d90e48790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d197dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9988f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846babc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bcf6364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:57.321026Z",
     "start_time": "2023-10-30T21:49:57.293657Z"
    },
    "code_folding": [
     70
    ]
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, value=None, next=None, skip_next=None, tf=0):\n",
    "        \"\"\" Class to define the structure of each node in a linked list (postings list).\n",
    "            Value: document id, Next: Pointer to the next node\n",
    "            Add more parameters if needed.\n",
    "            Hint: You may want to define skip pointers & appropriate score calculation here\"\"\"\n",
    "        self.value = value #document-ID\n",
    "        self.next = next #next pointer\n",
    "        self.skip_next = skip_next #skip pointer\n",
    "        self.tf = tf #tf score for each term,doc pair\n",
    "        \n",
    "    def increase_tf_count(self):\n",
    "        self.tf = self.tf+1\n",
    "\n",
    "\n",
    "class LinkedList:\n",
    "    \"\"\" Class to define a linked list (postings list). Each element in the linked list is of the type 'Node'\n",
    "        Each term in the inverted index has an associated linked list object.\n",
    "        Feel free to add additional functions to this class.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_node = None #start of PL\n",
    "        self.end_node = None #end of PL\n",
    "        self.length, self.n_skips = 0, 0 #length of PL, no of skips, term freq\n",
    "        self.skip_length = None #skip length\n",
    "        \n",
    "    def traverse_list(self):\n",
    "        \n",
    "        if self.start_node is None:\n",
    "            return [\"List is empty\"]\n",
    "        \n",
    "            \"\"\" Write logic to traverse the linked list.\n",
    "                To be implemented.\"\"\"\n",
    "        traversal = []\n",
    "        tf_scores = []\n",
    "        curr = self.start_node\n",
    "        traversal.append(curr.value)\n",
    "        tf_scores.append(curr.tf)\n",
    "        while(curr.next):\n",
    "            traversal.append(curr.next.value)\n",
    "            tf_scores.append(curr.next.tf)\n",
    "            curr = curr.next\n",
    "        return traversal,tf_scores\n",
    "\n",
    "    def traverse_skips(self):\n",
    "        traversal = []\n",
    "        if self.start_node is None:\n",
    "            return\n",
    "        else:\n",
    "            \"\"\" Write logic to traverse the linked list using skip pointers.\n",
    "                To be implemented.\"\"\"\n",
    "            curr = self.start_node\n",
    "            if(curr.skip_next):\n",
    "                traversal.append(curr.value)\n",
    "            while(curr.skip_next):\n",
    "                traversal.append(curr.skip_next.value)\n",
    "                curr = curr.skip_next\n",
    "            return traversal\n",
    "\n",
    "    def add_skip_connections(self):\n",
    "        if(self.length < 3):\n",
    "            return\n",
    "        \n",
    "        \"\"\" Write logic to add skip pointers to the linked list. \n",
    "            This function does not return anything.\n",
    "            To be implemented.\"\"\"\n",
    "        \n",
    "        n_skips = math.floor(math.sqrt(self.length))\n",
    "        if n_skips * n_skips == self.length:\n",
    "            n_skips = n_skips - 1\n",
    "        \n",
    "        self.n_skips = n_skips\n",
    "        self.skip_length = int(np.round(math.sqrt(self.length),0))\n",
    "        \n",
    "        prev = None\n",
    "        curr = self.start_node\n",
    "        while(curr):\n",
    "            count = 0\n",
    "            prev = curr\n",
    "            while((curr.next is not None) & (count < self.skip_length)):\n",
    "                curr = curr.next\n",
    "                count += 1\n",
    "            if(count == self.skip_length):\n",
    "                prev.skip_next = curr \n",
    "            if(curr.next is None):\n",
    "                break\n",
    "\n",
    "    def insert(self, value):\n",
    "        \"\"\" Write logic to add new elements to the linked list.\n",
    "            Insert the element at an appropriate position, such that elements to the left are lower than the inserted\n",
    "            element, and elements to the right are greater than the inserted element.\n",
    "            To be implemented. \"\"\"\n",
    "        new_node = Node(value,None,None,1)\n",
    "        prev_node = None\n",
    "        curr_node = self.start_node\n",
    "        \n",
    "        if(self.length == 0 or curr_node is None):\n",
    "            self.start_node = self.end_node = new_node\n",
    "        else:\n",
    "            while((curr_node.next is not None) & (curr_node.value < value)):\n",
    "                prev_node = curr_node\n",
    "                curr_node = curr_node.next\n",
    "            if(curr_node.value > value):\n",
    "                if(prev_node is None):\n",
    "                    self.start_node = new_node\n",
    "                    self.start_node.next = curr_node\n",
    "                else:\n",
    "                    prev_node.next = new_node\n",
    "                    new_node.next = curr_node\n",
    "            elif(curr_node.value == value):\n",
    "                curr_node.increase_tf_count()\n",
    "                self.length -= 1\n",
    "            else :\n",
    "                new_node.next = curr_node.next\n",
    "                curr_node.next = new_node\n",
    "                if(new_node.next is None):\n",
    "                    self.end_node = new_node\n",
    "        self.length += 1  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d811f761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:49:57.600795Z",
     "start_time": "2023-10-30T21:49:57.591622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LinkedList at 0x7f7d91195ee0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = LinkedList()\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ea45c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:03.658161Z",
     "start_time": "2023-10-30T21:50:03.644710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = LinkedList()\n",
    "elms_to_insert = [7,8,9]\n",
    "for el in elms_to_insert:\n",
    "    ll.insert(el)\n",
    "ll.add_skip_connections()\n",
    "ll.traverse_skips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e28910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T05:19:27.126993Z",
     "start_time": "2023-10-27T05:19:27.118432Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b860d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b29bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:09.079831Z",
     "start_time": "2023-10-30T21:50:09.070314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ([7, 8, 9, 10], [1, 1, 1, 1]) 7 10 4\n",
      "15 ([7, 8, 9, 10, 15], [1, 1, 1, 1, 1]) 7 15 5\n",
      "5 ([5, 7, 8, 9, 10, 15], [1, 1, 1, 1, 1, 1]) 5 15 6\n",
      "1 ([1, 5, 7, 8, 9, 10, 15], [1, 1, 1, 1, 1, 1, 1]) 1 15 7\n",
      "25 ([1, 5, 7, 8, 9, 10, 15, 25], [1, 1, 1, 1, 1, 1, 1, 1]) 1 25 8\n",
      "12 ([1, 5, 7, 8, 9, 10, 12, 15, 25], [1, 1, 1, 1, 1, 1, 1, 1, 1]) 1 25 9\n",
      "12 ([1, 5, 7, 8, 9, 10, 12, 15, 25], [1, 1, 1, 1, 1, 1, 2, 1, 1]) 1 25 9\n",
      "14 ([1, 5, 7, 8, 9, 10, 12, 14, 15, 25], [1, 1, 1, 1, 1, 1, 2, 1, 1, 1]) 1 25 10\n",
      "15 ([1, 5, 7, 8, 9, 10, 12, 14, 15, 25], [1, 1, 1, 1, 1, 1, 2, 1, 2, 1]) 1 25 10\n",
      "35 ([1, 5, 7, 8, 9, 10, 12, 14, 15, 25, 35], [1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1]) 1 35 11\n",
      "35 ([1, 5, 7, 8, 9, 10, 12, 14, 15, 25, 35], [1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2]) 1 35 11\n",
      "30 ([1, 5, 7, 8, 9, 10, 12, 14, 15, 25, 30, 35], [1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2]) 1 35 12\n",
      "50 ([1, 5, 7, 8, 9, 10, 12, 14, 15, 25, 30, 35, 50], [1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1]) 1 50 13\n"
     ]
    }
   ],
   "source": [
    "elms_to_insert = [10,15,5,1,25,12,12,14,15,35,35,30,50]\n",
    "for el in elms_to_insert:\n",
    "    ll.insert(el)\n",
    "    print(el,ll.traverse_list(), ll.start_node.value,ll.end_node.value, ll.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e3366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91d5df17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:14.319540Z",
     "start_time": "2023-10-30T21:50:14.313521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "5 1\n",
      "7 1\n",
      "8 1\n",
      "9 1\n",
      "10 1\n",
      "12 2\n",
      "14 1\n",
      "15 2\n",
      "25 1\n",
      "30 1\n",
      "35 2\n",
      "50 1\n"
     ]
    }
   ],
   "source": [
    "p = ll.start_node\n",
    "while(p):\n",
    "    print(p.value, p.tf)\n",
    "    p = p.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a0efcc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:16.314711Z",
     "start_time": "2023-10-30T21:50:16.306408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8425edb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:16.599449Z",
     "start_time": "2023-10-30T21:50:16.591417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elms_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97f7a7fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:16.871714Z",
     "start_time": "2023-10-30T21:50:16.861175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 5, 7, 8, 9, 10, 12, 14, 15, 25, 30, 35, 50],\n",
       " [1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.traverse_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da307c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:17.176540Z",
     "start_time": "2023-10-30T21:50:17.167386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 9, 15, 50]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.add_skip_connections()\n",
    "ll.traverse_skips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "069ec495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:17.635103Z",
     "start_time": "2023-10-30T21:50:17.630675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662c9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473d8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3740d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96a2a3a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:20.068436Z",
     "start_time": "2023-10-30T21:50:20.045873Z"
    }
   },
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    def __init__(self):\n",
    "        \"\"\" Add more attributes if needed\"\"\"\n",
    "        self.inverted_index = OrderedDict({}) #contains key:term, value:(length of PL, PL)\n",
    "        self.doc_token_counts = OrderedDict({}) #no of tokens in a document \n",
    "        self.tf_idf_scores = OrderedDict({}) #tf-idf scores for each (term,doc) pair\n",
    "        \n",
    "    def get_index(self):\n",
    "        \"\"\" Function to get the index.\n",
    "            Already implemented.\"\"\"\n",
    "        return self.inverted_index\n",
    "    \n",
    "    def get_doc_token_counts(self):\n",
    "        \"\"\" Function to get the index.\n",
    "            Already implemented.\"\"\"\n",
    "        return self.doc_token_counts\n",
    "\n",
    "    def get_tf_idf_scores(self):\n",
    "        \"\"\" Function to get the index.\n",
    "            Already implemented.\"\"\"\n",
    "        return self.tf_idf_scores\n",
    "    \n",
    "    def generate_inverted_index(self, doc_id, tokenized_document):\n",
    "        \"\"\" This function adds each tokenized document to the index. This in turn uses the function add_to_index\n",
    "            Already implemented.\"\"\"\n",
    "        for term_ in tokenized_document:\n",
    "            self.add_to_index(term_, doc_id)\n",
    "        self.doc_token_counts[doc_id] = len(tokenized_document)\n",
    "\n",
    "    def add_to_index(self, term_, doc_id_):\n",
    "        \n",
    "        \"\"\" This function adds each term & document id to the index.\n",
    "            If a term is not present in the index, then add the term to the index & initialize a new postings list (linked list).\n",
    "            If a term is present, then add the document to the appropriate position in the posstings list of the term.\n",
    "            To be implemented.\"\"\"\n",
    "        if term_ not in self.inverted_index.keys():\n",
    "            self.inverted_index[term_] = [0,LinkedList()]\n",
    "            \n",
    "        lpl = self.inverted_index[term_][0]\n",
    "        self.inverted_index[term_][0] = lpl + 1\n",
    "        self.inverted_index[term_][1].insert(doc_id_)\n",
    "\n",
    "    def sort_terms(self):\n",
    "        \"\"\" Sorting the index by terms.\n",
    "            Already implemented.\"\"\"\n",
    "        sorted_index = OrderedDict({})\n",
    "        for k in sorted(self.inverted_index.keys()):\n",
    "            sorted_index[k] = self.inverted_index[k]\n",
    "        self.inverted_index = sorted_index\n",
    "\n",
    "    def add_skip_connections(self):\n",
    "        \"\"\" For each postings list in the index, add skip pointers.\n",
    "            To be implemented.\"\"\"\n",
    "        for term_ in self.inverted_index.keys():\n",
    "            self.inverted_index[term_][1].add_skip_connections()\n",
    "\n",
    "    def calculate_tf(self,n_term,n_doc):\n",
    "        return n_term/n_doc\n",
    "            \n",
    "    def calculate_idf(self, D, N):\n",
    "        if D == 0:\n",
    "            return 0 \n",
    "        return math.log(N / D)\n",
    "            \n",
    "    def calculate_tf_idf(self):\n",
    "        \"\"\" Calculate tf-idf score for each document in the postings lists of the index.\n",
    "            To be implemented.\"\"\"\n",
    "        tf_idf_scores = {}\n",
    "        N = len(self.doc_token_counts)\n",
    "        for term_ in self.inverted_index.keys():\n",
    "            D = self.inverted_index[term_][0]\n",
    "            idf_score = self.calculate_idf(D,N)\n",
    "    \n",
    "            PL,tf_counts = self.inverted_index[term_][1].traverse_list() \n",
    "            for i in range(len(PL)):\n",
    "                n_term =  tf_counts[i]\n",
    "                n_doc = self.doc_token_counts[PL[i]]\n",
    "                tf_score = self.calculate_tf(n_term,n_doc)\n",
    "                tf_idf_scores[(term_,PL[i])] = np.round(tf_score * idf_score, 2)\n",
    "        self.tf_idf_scores = tf_idf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a9beec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:20.566961Z",
     "start_time": "2023-10-30T21:50:20.559063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Indexer at 0x7f7d9124fdc0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = Indexer()\n",
    "indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e35a2657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:21.031365Z",
     "start_time": "2023-10-30T21:50:21.022664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.get_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8eae258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:21.246646Z",
     "start_time": "2023-10-30T21:50:21.238990Z"
    }
   },
   "outputs": [],
   "source": [
    "indexer.generate_inverted_index(1, ['t1','t2','t3'])\n",
    "indexer.generate_inverted_index(2, ['t1','t1','t2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f897750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:21.514541Z",
     "start_time": "2023-10-30T21:50:21.505494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('t1', [3, <__main__.LinkedList at 0x7f7d80285b80>]),\n",
       "             ('t2', [2, <__main__.LinkedList at 0x7f7d80285640>]),\n",
       "             ('t3', [1, <__main__.LinkedList at 0x7f7d80285dc0>])])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c675537d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:22.091432Z",
     "start_time": "2023-10-30T21:50:22.082139Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2], [1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.inverted_index['t1'][1].traverse_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a96b8658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:22.405960Z",
     "start_time": "2023-10-30T21:50:22.397245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, 3), (2, 3)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.doc_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c68f4c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:22.696678Z",
     "start_time": "2023-10-30T21:50:22.690504Z"
    }
   },
   "outputs": [],
   "source": [
    "indexer.calculate_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600aca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7a0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f34ed8d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:23.833854Z",
     "start_time": "2023-10-30T21:50:23.825764Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge(pl_1, pl_2):\n",
    "    i,j,m,n = 0,0,len(pl_1),len(pl_2)\n",
    "    pl_merged = []\n",
    "    while((i < m) & (j < n)):\n",
    "        if(pl_1[i] == pl_2[j]):\n",
    "            pl_merged.append(pl_1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif(pl_1[i] < pl_2[j]):\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return pl_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a80a4600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:24.113575Z",
     "start_time": "2023-10-30T21:50:24.104802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b = [2,3]\n",
    "merge(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5a7730b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:24.292164Z",
     "start_time": "2023-10-30T21:50:24.285718Z"
    }
   },
   "outputs": [],
   "source": [
    "def daat_and(terms):\n",
    "    '''\n",
    "    terms = list(set(terms))\n",
    "    PL_values,PL_counts = [],[]\n",
    "    for term in terms:\n",
    "        index = indexer.get_index()\n",
    "        PL = [i for i in range(10)]#index[term].traverse_list()\n",
    "        PL_values.append(PL)\n",
    "        PL_counts.append(len(PL))\n",
    "    '''\n",
    "    PL_counts = [5,4,6,2,3]\n",
    "    PL_values = [[1,2,3,4,5],\n",
    "                 [4,5,6,10],\n",
    "                 [1,3,4,7,10,16],\n",
    "                 [1,4],\n",
    "                 []\n",
    "                ]\n",
    "    \n",
    "    \n",
    "    all_PL = list(zip(PL_counts, PL_values))\n",
    "    all_PL_sorted = sorted(all_PL, key=lambda x: x[0])\n",
    "    PL_counts, PL_values  = zip(*all_PL_sorted)\n",
    "    \n",
    "    min_PL = PL_values[0]\n",
    "    for i in range(1,len(PL_values)):\n",
    "        min_PL = merge(min_PL, PL_values[i])\n",
    "        \n",
    "    return min_PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7df89813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:50:24.735941Z",
     "start_time": "2023-10-30T21:50:24.727567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = ['t1','t2','t3']\n",
    "daat_and(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e1b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a6f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ca7e709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:52:03.451116Z",
     "start_time": "2023-10-30T21:52:03.406302Z"
    },
    "code_folding": [
     5,
     25,
     61,
     97,
     133,
     136,
     142
    ]
   },
   "outputs": [],
   "source": [
    "class ProjectRunner:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.indexer = Indexer()\n",
    "\n",
    "    def _merge(self, pl_1, pl_2):\n",
    "        \"\"\" Implement the merge algorithm to merge 2 postings list at a time.\n",
    "            Use appropriate parameters & return types.\n",
    "            While merging 2 postings list, preserve the maximum tf-idf value of a document.\n",
    "            To be implemented.\"\"\"\n",
    "        i,j,m,n = 0,0,len(pl_1),len(pl_2)\n",
    "        pl_merged = []\n",
    "        num_comparisions = 0\n",
    "        while((i < m) & (j < n)):\n",
    "            num_comparisions += 1\n",
    "            if(pl_1[i] == pl_2[j]):\n",
    "                pl_merged.append(pl_1[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif(pl_1[i] < pl_2[j]):\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        return pl_merged,num_comparisions\n",
    "\n",
    "    def _daat_and(self, terms, score=0):\n",
    "        \"\"\" Implement the DAAT AND algorithm, which merges the postings list of N query terms.\n",
    "            Use appropriate parameters & return types.\n",
    "            To be implemented.\"\"\"\n",
    "        terms = list(set(terms))\n",
    "        PL_values,PL_counts = [],[]\n",
    "        for term in terms:\n",
    "            PL,_ = self._get_postings(term)\n",
    "            PL_values.append(PL)\n",
    "            PL_counts.append(len(PL))\n",
    "            \n",
    "        all_PL = list(zip(PL_counts, PL_values))\n",
    "        all_PL_sorted = sorted(all_PL, key=lambda x: x[0])\n",
    "        PL_counts, PL_values  = zip(*all_PL_sorted)\n",
    "\n",
    "        min_PL = PL_values[0]\n",
    "        total_comparisions = 0\n",
    "        for i in range(1,len(PL_values)):\n",
    "            min_PL,num_comparisions = self._merge(min_PL, PL_values[i])\n",
    "            total_comparisions += num_comparisions\n",
    "\n",
    "        if((score) & (len(min_PL) > 1)):\n",
    "            doc_scores = []\n",
    "            for doc in min_PL:\n",
    "                total_score = 0\n",
    "                for term in terms:\n",
    "                    s = self.indexer.get_tf_idf_scores()[(term,doc)]\n",
    "                    total_score += s \n",
    "                doc_scores.append(total_score)\n",
    "            answer_score = list(zip(min_PL, doc_scores))\n",
    "            answer_score_sorted = sorted(answer_score, key=lambda x: x[1],reverse=True)\n",
    "            answer_score, doc_scores  = zip(*answer_score_sorted)  \n",
    "            return answer_score,total_comparisions\n",
    "    \n",
    "        return min_PL,total_comparisions\n",
    "\n",
    "\n",
    "    def _get_postings(self,term):\n",
    "        \"\"\" Function to get the postings list of a term from the index.\n",
    "            Use appropriate parameters & return types.\n",
    "            To be implemented.\"\"\"\n",
    "        return self.indexer.get_index()[term][1].traverse_list()\n",
    "\n",
    "    def _merge_skip(self, pl_1, pl_2):\n",
    "        \"\"\" Implement the merge algorithm to merge 2 postings list at a time.\n",
    "            Use appropriate parameters & return types.\n",
    "            While merging 2 postings list, preserve the maximum tf-idf value of a document.\n",
    "            To be implemented.\"\"\"\n",
    "        pl_merged = []\n",
    "        num_comparisions = 0\n",
    "        while((pl_1 is not None) & (pl_2 is not None)):\n",
    "            num_comparisions += 1\n",
    "            if(pl_1.value == pl_2.value):\n",
    "                pl_merged.append(pl_1.value)\n",
    "                pl_1 = pl_1.next\n",
    "                pl_2 = pl_2.next\n",
    "                \n",
    "            elif(pl_1.value < pl_2.value):\n",
    "                if(pl_1.skip_next is not None) and (pl_1.skip_next.value <= pl_2.value):\n",
    "                    while(pl_1.skip_next is not None) and (pl_1.skip_next.value <= pl_2.value):\n",
    "                        pl_1 = pl_1.skip_next\n",
    "                else:\n",
    "                    pl_1 = pl_1.next\n",
    "                  \n",
    "            else:\n",
    "                if(pl_2.skip_next is not None) and (pl_2.skip_next.value <= pl_1.value):  \n",
    "                    while(pl_2.skip_next is not None) and (pl_2.skip_next.value <= pl_1.value):\n",
    "                        pl_2 = pl_2.skip_next\n",
    "                else:\n",
    "                    pl_2 = pl_2.next\n",
    "                    \n",
    "        return pl_merged,num_comparisions\n",
    "    \n",
    "    def _daat_and_skip(self, terms, score=0):\n",
    "        \"\"\" Implement the DAAT AND algorithm, which merges the postings list of N query terms.\n",
    "            Use appropriate parameters & return types.\n",
    "            To be implemented.\"\"\"\n",
    "        terms = list(set(terms))\n",
    "        start_ptrs,PL_counts = [],[]\n",
    "        for term in terms:\n",
    "            p = self.indexer.get_index()[term][1].start_node\n",
    "            l = self.indexer.get_index()[term][1].length\n",
    "            start_ptrs.append(p)\n",
    "            PL_counts.append(l)\n",
    "            \n",
    "        all_PL = list(zip(PL_counts, start_ptrs))\n",
    "        all_PL_sorted = sorted(all_PL, key=lambda x: x[0])\n",
    "        PL_counts, start_ptrs  = zip(*all_PL_sorted)\n",
    "\n",
    "        min_start_ptr = start_ptrs[0]\n",
    "        total_comparisions = 0\n",
    "        for i in range(1,len(start_ptrs)):\n",
    "            answer,num_comparisions = self._merge_skip(min_start_ptr, start_ptrs[i])\n",
    "            total_comparisions += num_comparisions\n",
    "            \n",
    "        if((score) & (len(answer) > 1)):\n",
    "            doc_scores = []\n",
    "            for doc in answer:\n",
    "                total_score = 0\n",
    "                for term in terms:\n",
    "                    total_score += self.indexer.get_tf_idf_scores()[(term,doc)]\n",
    "                doc_scores.append(total_score)\n",
    "            answer_score = list(zip(answer, doc_scores))\n",
    "            answer_score_sorted = sorted(answer_score, key=lambda x: x[1],reverse=True)\n",
    "            answer_score, doc_scores  = zip(*answer_score_sorted) \n",
    "            return answer_score,total_comparisions\n",
    "\n",
    "        return answer,total_comparisions\n",
    "    \n",
    "\n",
    "    def _output_formatter(self, op):\n",
    "        \"\"\" This formats the result in the required format.\n",
    "            Do NOT change.\"\"\"\n",
    "        if op is None or len(op) == 0:\n",
    "            return [], 0\n",
    "        op_no_score = [int(i) for i in op]\n",
    "        results_cnt = len(op_no_score)\n",
    "        return op_no_score, results_cnt\n",
    "\n",
    "    def run_indexer(self, corpus):\n",
    "        \"\"\" This function reads & indexes the corpus. After creating the inverted index,\n",
    "            it sorts the index by the terms, add skip pointers, and calculates the tf-idf scores.\n",
    "            Already implemented, but you can modify the orchestration, as you seem fit.\"\"\"\n",
    "        with open(corpus, 'r') as fp:\n",
    "            for line in tqdm(fp.readlines()):\n",
    "                doc_id, document = self.preprocessor.get_doc_id(line)\n",
    "                tokenized_document = self.preprocessor.tokenizer(document)\n",
    "                self.indexer.generate_inverted_index(doc_id, tokenized_document)\n",
    "        self.indexer.sort_terms()\n",
    "        self.indexer.add_skip_connections()\n",
    "        self.indexer.calculate_tf_idf()\n",
    "\n",
    "    def sanity_checker(self, command):\n",
    "        \"\"\" DO NOT MODIFY THIS. THIS IS USED BY THE GRADER. \"\"\"\n",
    "\n",
    "        index = self.indexer.get_index()\n",
    "        kw = random.choice(list(index.keys()))\n",
    "        return {\"index_type\": str(type(index)),\n",
    "                \"indexer_type\": str(type(self.indexer)),\n",
    "                \"post_mem\": str(index[kw]),\n",
    "                \"post_type\": str(type(index[kw])),\n",
    "                \"node_mem\": str(index[kw].start_node),\n",
    "                \"node_type\": str(type(index[kw].start_node)),\n",
    "                \"node_value\": str(index[kw].start_node.value),\n",
    "                \"command_result\": eval(command) if \".\" in command else \"\"}\n",
    "\n",
    "    def run_queries(self, query_list, random_command):\n",
    "        \"\"\" DO NOT CHANGE THE output_dict definition\"\"\"\n",
    "        output_dict = {'postingsList': {},\n",
    "                       'postingsListSkip': {},\n",
    "                       'daatAnd': {},\n",
    "                       'daatAndSkip': {},\n",
    "                       'daatAndTfIdf': {},\n",
    "                       'daatAndSkipTfIdf': {},\n",
    "                       'sanity': {}#self.sanity_checker(random_command)\n",
    "                      }\n",
    "\n",
    "        for query in tqdm(query_list):\n",
    "            \"\"\" Run each query against the index. You should do the following for each query:\n",
    "                1. Pre-process & tokenize the query.\n",
    "                2. For each query token, get the postings list & postings list with skip pointers.\n",
    "                3. Get the DAAT AND query results & number of comparisons with & without skip pointers.\n",
    "                4. Get the DAAT AND query results & number of comparisons with & without skip pointers, \n",
    "                    along with sorting by tf-idf scores.\"\"\"\n",
    "            \n",
    "            input_term_arr = self.preprocessor.tokenizer(query)  # Tokenized query. To be implemented.\n",
    "\n",
    "            for term in input_term_arr:\n",
    "                postings,scores = self.indexer.get_index()[term][1].traverse_list()\n",
    "                skip_postings = self.indexer.get_index()[term][1].traverse_skips()\n",
    "\n",
    "\n",
    "                \"\"\" Implement logic to populate initialize the above variables.\n",
    "                    The below code formats your result to the required format.\n",
    "                    To be implemented.\"\"\"\n",
    "\n",
    "                output_dict['postingsList'][term] = postings\n",
    "                output_dict['postingsListSkip'][term] = skip_postings\n",
    "\n",
    "            and_op_no_skip,and_comparisons_no_skip = self._daat_and(input_term_arr,score=0)\n",
    "            and_op_skip,and_comparisons_skip = self._daat_and_skip(input_term_arr,score=0) \n",
    "            and_op_no_skip_sorted,and_comparisons_no_skip_sorted = self._daat_and(input_term_arr,score=1) \n",
    "            and_op_skip_sorted,and_comparisons_skip_sorted = self._daat_and_skip(input_term_arr,score=1) \n",
    "            \n",
    "            \"\"\" Implement logic to populate initialize the above variables.\n",
    "                The below code formats your result to the required format.\n",
    "                To be implemented.\"\"\"\n",
    "            and_op_no_score_no_skip, and_results_cnt_no_skip = self._output_formatter(and_op_no_skip)\n",
    "            and_op_no_score_skip, and_results_cnt_skip = self._output_formatter(and_op_skip)\n",
    "            and_op_no_score_no_skip_sorted, and_results_cnt_no_skip_sorted = self._output_formatter(and_op_no_skip_sorted)\n",
    "            and_op_no_score_skip_sorted, and_results_cnt_skip_sorted = self._output_formatter(and_op_skip_sorted)\n",
    "            \n",
    "            output_dict['daatAnd'][query.strip()] = {}\n",
    "            output_dict['daatAnd'][query.strip()]['results'] = and_op_no_score_no_skip\n",
    "            output_dict['daatAnd'][query.strip()]['num_docs'] = and_results_cnt_no_skip\n",
    "            output_dict['daatAnd'][query.strip()]['num_comparisons'] = and_comparisons_no_skip\n",
    "\n",
    "            output_dict['daatAndSkip'][query.strip()] = {}\n",
    "            output_dict['daatAndSkip'][query.strip()]['results'] = and_op_no_score_skip\n",
    "            output_dict['daatAndSkip'][query.strip()]['num_docs'] = and_results_cnt_skip\n",
    "            output_dict['daatAndSkip'][query.strip()]['num_comparisons'] = and_comparisons_skip\n",
    "\n",
    "            output_dict['daatAndTfIdf'][query.strip()] = {}\n",
    "            output_dict['daatAndTfIdf'][query.strip()]['results'] = and_op_no_score_no_skip_sorted\n",
    "            output_dict['daatAndTfIdf'][query.strip()]['num_docs'] = and_results_cnt_no_skip_sorted\n",
    "            output_dict['daatAndTfIdf'][query.strip()]['num_comparisons'] = and_comparisons_no_skip_sorted\n",
    "\n",
    "            output_dict['daatAndSkipTfIdf'][query.strip()] = {}\n",
    "            output_dict['daatAndSkipTfIdf'][query.strip()]['results'] = and_op_no_score_skip_sorted\n",
    "            output_dict['daatAndSkipTfIdf'][query.strip()]['num_docs'] = and_results_cnt_skip_sorted\n",
    "            output_dict['daatAndSkipTfIdf'][query.strip()]['num_comparisons'] = and_comparisons_skip_sorted\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05c24bc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:52:03.917854Z",
     "start_time": "2023-10-30T21:52:03.905978Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ProjectRunner at 0x7f7db39c1760>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = ProjectRunner()\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8ec1e87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:52:04.179836Z",
     "start_time": "2023-10-30T21:52:04.165035Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 5955.00it/s]\n"
     ]
    }
   ],
   "source": [
    "pr.run_indexer(\"./data/sample_corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa96f669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:52:04.432552Z",
     "start_time": "2023-10-30T21:52:04.417628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('back', [1, <__main__.LinkedList at 0x7f7db39b88b0>]),\n",
       "             ('cafe', [1, <__main__.LinkedList at 0x7f7db39c1490>]),\n",
       "             ('common', [1, <__main__.LinkedList at 0x7f7db39c1f70>]),\n",
       "             ('even', [1, <__main__.LinkedList at 0x7f7db39c18b0>]),\n",
       "             ('go', [2, <__main__.LinkedList at 0x7f7db39c12e0>]),\n",
       "             ('good', [1, <__main__.LinkedList at 0x7f7db39c1d60>]),\n",
       "             ('health', [1, <__main__.LinkedList at 0x7f7db39c15e0>]),\n",
       "             ('hello', [5, <__main__.LinkedList at 0x7f7db39ab280>]),\n",
       "             ('hi', [1, <__main__.LinkedList at 0x7f7d913e0580>]),\n",
       "             ('jack', [2, <__main__.LinkedList at 0x7f7db39c1670>]),\n",
       "             ('know', [1, <__main__.LinkedList at 0x7f7db39c1250>]),\n",
       "             ('let', [1, <__main__.LinkedList at 0x7f7db39c1370>]),\n",
       "             ('meet', [2, <__main__.LinkedList at 0x7f7db39c1a00>]),\n",
       "             ('monday', [1, <__main__.LinkedList at 0x7f7db39c1e80>]),\n",
       "             ('random', [3, <__main__.LinkedList at 0x7f7d9140e250>]),\n",
       "             ('randomli', [1, <__main__.LinkedList at 0x7f7db39b8580>]),\n",
       "             ('strike', [1, <__main__.LinkedList at 0x7f7db39b8550>]),\n",
       "             ('swim', [3, <__main__.LinkedList at 0x7f7db39c1970>]),\n",
       "             ('text', [4, <__main__.LinkedList at 0x7f7d9140efd0>]),\n",
       "             ('thing', [1, <__main__.LinkedList at 0x7f7db39c1a90>]),\n",
       "             ('world', [3, <__main__.LinkedList at 0x7f7d913f7d90>])])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.indexer.get_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee284dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:52:04.868298Z",
     "start_time": "2023-10-30T21:52:04.862504Z"
    }
   },
   "outputs": [],
   "source": [
    "random_command = \"random\"\n",
    "query_list = [\"hello world\",\n",
    "              \"hello swimming\", \n",
    "              \"swimming going\", \n",
    "              \"random swimming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9150b577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T21:52:05.332300Z",
     "start_time": "2023-10-30T21:52:05.315398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 4938.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'postingsList': {'hello': [1, 2, 5, 9],\n",
       "  'world': [1, 2],\n",
       "  'swim': [7, 8, 9],\n",
       "  'go': [7, 9],\n",
       "  'random': [10, 11, 12]},\n",
       " 'postingsListSkip': {'hello': [1, 5],\n",
       "  'world': [],\n",
       "  'swim': [7, 9],\n",
       "  'go': [],\n",
       "  'random': [10, 12]},\n",
       " 'daatAnd': {'hello world': {'results': [1, 2],\n",
       "   'num_docs': 2,\n",
       "   'num_comparisons': 2},\n",
       "  'hello swimming': {'results': [9], 'num_docs': 1, 'num_comparisons': 6},\n",
       "  'swimming going': {'results': [7, 9], 'num_docs': 2, 'num_comparisons': 3},\n",
       "  'random swimming': {'results': [], 'num_docs': 0, 'num_comparisons': 3}},\n",
       " 'daatAndSkip': {'hello world': {'results': [1, 2],\n",
       "   'num_docs': 2,\n",
       "   'num_comparisons': 2},\n",
       "  'hello swimming': {'results': [9], 'num_docs': 1, 'num_comparisons': 4},\n",
       "  'swimming going': {'results': [7, 9], 'num_docs': 2, 'num_comparisons': 3},\n",
       "  'random swimming': {'results': [], 'num_docs': 0, 'num_comparisons': 2}},\n",
       " 'daatAndTfIdf': {'hello world': {'results': [1, 2],\n",
       "   'num_docs': 2,\n",
       "   'num_comparisons': 2},\n",
       "  'hello swimming': {'results': [9], 'num_docs': 1, 'num_comparisons': 6},\n",
       "  'swimming going': {'results': [9, 7], 'num_docs': 2, 'num_comparisons': 3},\n",
       "  'random swimming': {'results': [], 'num_docs': 0, 'num_comparisons': 3}},\n",
       " 'daatAndSkipTfIdf': {'hello world': {'results': [1, 2],\n",
       "   'num_docs': 2,\n",
       "   'num_comparisons': 2},\n",
       "  'hello swimming': {'results': [9], 'num_docs': 1, 'num_comparisons': 4},\n",
       "  'swimming going': {'results': [9, 7], 'num_docs': 2, 'num_comparisons': 3},\n",
       "  'random swimming': {'results': [], 'num_docs': 0, 'num_comparisons': 2}},\n",
       " 'sanity': {}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.run_queries(query_list, random_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3268c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d24529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
